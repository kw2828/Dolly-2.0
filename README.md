# 🛡️ Safeguards Shield 
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PyPI - Python Version](https://img.shields.io/pypi/v/llm-guard)](https://pypi.org/project/guardrail-ml)
[![Downloads](https://static.pepy.tech/badge/guardrail-ml)](https://pepy.tech/project/guardrail-ml)

![plot](./static/images/safeguards-shield.png)

Safeguards Shield is a developer toolkit to use LLMs safely and securely. Our Shield safeguards prompts and LLM interactions from costly risks to bring your AI app from prototype to production faster with confidence.

Our Shield wraps your GenAI apps with a protective layer, safeguarding malicious inputs and filtering model outputs. Our comprehensive toolkit has 20+ out-of-the-box detectors for robust protection of your GenAI apps in workflow.


## Benefits
- 🚀 mitigate LLM reliability and safety risks 
- 📝 customize and ensure LLM behaviors are safe and secure
- 💸 monitor incidents, costs, and responsible AI metrics 

## Features 
- 🛠️ shield that safeguards against CVEs and improves with each attack
- 🤖 reduce and measure ungrounded additions (hallucinations) with tools
- 🛡️ multi-layered defense with heuristic detectors, LLM-based, vector DB
